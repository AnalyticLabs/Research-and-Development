{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Roberta retraining.ipynb","provenance":[{"file_id":"14gTSOHd6R3Zf9D7U3yqJAjdmahAVrpzj","timestamp":1575141702531},{"file_id":"1VvbuWCmHHnOsgwm-1_g_Yw1YuLQ7tw4i","timestamp":1575125326509}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2PnbgZhXfv2O","colab_type":"code","outputId":"339fdc45-4589-4184-dd04-53db1800c14d","executionInfo":{"status":"ok","timestamp":1575208384690,"user_tz":-330,"elapsed":4220,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z8KUQgNNFMcF","colab_type":"code","outputId":"1470a7c0-f134-41b3-e440-92c350757066","executionInfo":{"status":"ok","timestamp":1575208395662,"user_tz":-330,"elapsed":9758,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/e7/0a1babead1b79afabb654fbec0a052e0d833ba4205a6dfd98b1aeda9c82e/transformers-2.2.0-py3-none-any.whl (360kB)\n","\r\u001b[K     |█                               | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 81kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 92kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 133kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 143kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 153kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 163kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 174kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 184kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 194kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 204kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 215kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 225kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 235kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 245kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 256kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 266kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 276kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 286kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 296kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 307kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 317kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 327kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 337kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 348kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 358kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 368kB 6.6MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\r\u001b[K     |▎                               | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 32.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 35.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 37.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 39.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 41.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 41.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 43.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 44.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 44.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 44.2MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 44.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 44.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 44.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 44.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 44.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 44.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n","Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 43.0MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=24bdbb427e9d62bea7daa1d9523e2be1a9e33252f635e56a6a2908cec87872c7\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, regex, sacremoses, transformers\n","Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ecEdcodnf7Qq","colab_type":"code","outputId":"178f4db1-389f-4c10-cf41-9ca938391155","executionInfo":{"status":"ok","timestamp":1575208398530,"user_tz":-330,"elapsed":2837,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import RobertaTokenizer, RobertaConfig\n","from transformers import AdamW, RobertaForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","% matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aIv90vTvf-bk","colab_type":"code","outputId":"32c4193e-23c8-4075-f61e-deacbd8ebcb3","executionInfo":{"status":"ok","timestamp":1575208399086,"user_tz":-330,"elapsed":3379,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla K80'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"FJMi8LN7gACF","colab_type":"code","colab":{}},"source":["# Upload the train file from your local drive\n","# from google.colab import files\n","# uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5IqwI79e8pS","colab_type":"code","outputId":"acd30a85-7334-4ba5-aaf0-873f3ce4d963","executionInfo":{"status":"ok","timestamp":1575208419121,"user_tz":-330,"elapsed":23399,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/gdrive/')\n","%cd /gdrive"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive/\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RP4r9t9EgDhQ","colab_type":"code","colab":{}},"source":["# data = pd.read_csv('data.csv')\n","data = pd.read_csv('My Drive/Colab Notebooks/data.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzzlopIEihAR","colab_type":"code","outputId":"424c7e47-d2d4-45ac-ad05-43245d3e8e5d","executionInfo":{"status":"ok","timestamp":1575208423162,"user_tz":-330,"elapsed":2903,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["df = data[['Rating','Review_text']].rename(columns={'Rating': 'label', 'Review_text': 'sentence'})\n","df.label = df.label.astype(int)\n","df.label = df.label - 1\n","df = df.dropna()\n","df.sample(10)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>38876</th>\n","      <td>2</td>\n","      <td>Camera not good</td>\n","    </tr>\n","    <tr>\n","      <th>41282</th>\n","      <td>3</td>\n","      <td>Good TV, good picture, sound is little low</td>\n","    </tr>\n","    <tr>\n","      <th>34132</th>\n","      <td>4</td>\n","      <td>Aswam phone. Camera, battry all is good. But f...</td>\n","    </tr>\n","    <tr>\n","      <th>52763</th>\n","      <td>4</td>\n","      <td>good</td>\n","    </tr>\n","    <tr>\n","      <th>49043</th>\n","      <td>4</td>\n","      <td>I ordered this on a Saturday. It was delivered...</td>\n","    </tr>\n","    <tr>\n","      <th>7741</th>\n","      <td>4</td>\n","      <td>Good product at this range. Shows accurate det...</td>\n","    </tr>\n","    <tr>\n","      <th>20624</th>\n","      <td>4</td>\n","      <td>Awesome product...Good camera, battery...warp ...</td>\n","    </tr>\n","    <tr>\n","      <th>29399</th>\n","      <td>2</td>\n","      <td>Not up to mark</td>\n","    </tr>\n","    <tr>\n","      <th>41528</th>\n","      <td>2</td>\n","      <td>Battery is awesome...camera is okay has some g...</td>\n","    </tr>\n","    <tr>\n","      <th>50259</th>\n","      <td>4</td>\n","      <td>Best product at this price .good battery life ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       label                                           sentence\n","38876      2                                    Camera not good\n","41282      3         Good TV, good picture, sound is little low\n","34132      4  Aswam phone. Camera, battry all is good. But f...\n","52763      4                                               good\n","49043      4  I ordered this on a Saturday. It was delivered...\n","7741       4  Good product at this range. Shows accurate det...\n","20624      4  Awesome product...Good camera, battery...warp ...\n","29399      2                                     Not up to mark\n","41528      2  Battery is awesome...camera is okay has some g...\n","50259      4  Best product at this price .good battery life ..."]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"SNmg8bZBuYXO","colab_type":"text"},"source":["## Hyperparameters values as below\n","1. MAX_LEN = 128\n","2. learning_rate = 2e-5\n","3. batch_size = 32\n","4. epochs = 2\n","\n","##### Select a batch size for training. For fine-tuning on a specific task, recommend batch size is 16 or 32\n","##### Number of training epochs (recommend between 2 and 4)"]},{"cell_type":"code","metadata":{"id":"DiO6sbA8uwIb","colab_type":"code","colab":{}},"source":["MAX_LEN = 128\n","learning_rate = 2e-5\n","batch_size = 32\n","epochs = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hn7wy-tBoaCM","colab_type":"code","outputId":"64b8a806-e6af-45f3-a13a-b2538568fe8c","executionInfo":{"status":"ok","timestamp":1575208491818,"user_tz":-330,"elapsed":70486,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Create sentence and label lists\n","sentences_actual = df.sentence.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","sentences_actual = [\"[CLS] \" + sentences_actual + \" [SEP]\" for sentences_actual in sentences_actual]\n","labels_actual = df.label.values\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences_actual]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])\n","\n","index = []\n","for i,x in enumerate(tokenized_texts):\n","    if len(x)<MAX_LEN:\n","        index.append(i)\n","sentences = []\n","labels=[]\n","for i in index:\n","   sentences.append(sentences_actual[i]) \n","   labels.append(labels_actual[i])\n","print('total sentences are:',len(sentences))\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])\n","\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)\n","\n","# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n","\n","# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n","model.cuda()\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","\n","# This variable contains all of the hyperparemeter information our training loop needs\n","optimizer = AdamW(optimizer_grouped_parameters,\n","                     lr=learning_rate,\n","                     )\n","\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["100%|██████████| 898823/898823 [00:00<00:00, 1820246.85B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 1358357.38B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Tokenize the first sentence:\n","['[', 'CL', 'S', ']', 'ĠI', 'Ġliked', 'Ġit', 'Ġ[', 'SE', 'P', ']']\n","total sentences are: 58216\n","Tokenize the first sentence:\n","['[', 'CL', 'S', ']', 'ĠI', 'Ġliked', 'Ġit', 'Ġ[', 'SE', 'P', ']']\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 473/473 [00:00<00:00, 78820.25B/s]\n","100%|██████████| 501200538/501200538 [00:17<00:00, 28660034.11B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EVKDWGvYp3T4","colab_type":"code","outputId":"5125aee0-d81f-43c4-87d3-c52fc4563579","executionInfo":{"status":"ok","timestamp":1575206430598,"user_tz":-330,"elapsed":130426,"user":{"displayName":"arpana alka","photoUrl":"","userId":"00941466105270682854"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Store our loss and accuracy for plotting\n","train_loss_set = []\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  # Training\n","  \n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","    # Forward pass\n","    loss = model(b_input_ids,  attention_mask=b_input_mask, labels=b_labels)\n","\n","    train_loss_set.append(loss[0])    \n","    # # Backward pass\n","    loss[0].backward()\n","    # Update parameters and take a step using the computed gradient\n","    optimizer.step()\n","    \n","    \n","    # Update tracking variables\n","    tr_loss += loss[0]\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    \n","    \n","  # Validation\n","\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      logits = model(b_input_ids, attention_mask=b_input_mask)[0]\n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","\n","plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.8433193564414978\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  50%|█████     | 1/2 [38:57<38:57, 2337.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6750572344322344\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Ef8id-1v10c","colab_type":"text"},"source":["## Hyperparameters values as below\n","1. MAX_LEN = 128\n","2. learning_rate = 2e-5\n","3. batch_size = 16\n","4. epochs = 3"]},{"cell_type":"code","metadata":{"id":"UVNhVXXkvndk","colab_type":"code","colab":{}},"source":["MAX_LEN = 128\n","learning_rate = 2e-5\n","batch_size = 16\n","epochs = 3\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vjGUbKFwG7T","colab_type":"code","colab":{}},"source":["# Create sentence and label lists\n","sentences_actual = df.sentence.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","sentences_actual = [\"[CLS] \" + sentences_actual + \" [SEP]\" for sentences_actual in sentences_actual]\n","labels_actual = df.label.values\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences_actual]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])\n","\n","index = []\n","for i,x in enumerate(tokenized_texts):\n","    if len(x)<MAX_LEN:\n","        index.append(i)\n","sentences = []\n","labels=[]\n","for i in index:\n","   sentences.append(sentences_actual[i]) \n","   labels.append(labels_actual[i])\n","print('total sentences are:',len(sentences))\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])\n","\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)\n","\n","# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n","\n","# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n","model.cuda()\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","\n","# This variable contains all of the hyperparemeter information our training loop needs\n","optimizer = AdamW(optimizer_grouped_parameters,\n","                     lr=learning_rate,\n","                     )\n","\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo3jkWlHwN0t","colab_type":"code","colab":{}},"source":["# Store our loss and accuracy for plotting\n","train_loss_set = []\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  # Training\n","  \n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","    # Forward pass\n","    loss = model(b_input_ids,  attention_mask=b_input_mask, labels=b_labels)\n","\n","    train_loss_set.append(loss[0])    \n","    # # Backward pass\n","    loss[0].backward()\n","    # Update parameters and take a step using the computed gradient\n","    optimizer.step()\n","    \n","    \n","    # Update tracking variables\n","    tr_loss += loss[0]\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    \n","    \n","  # Validation\n","\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      logits = model(b_input_ids,  attention_mask=b_input_mask)[0]\n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","\n","plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDeBoX4iVdhB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}